/**\n * Enhanced Voice Assistant Client with Binary Audio and Performance Optimizations\n * \n * Features:\n * - Binary audio streaming for reduced latency\n * - Audio Worklet support for better performance\n * - VAD (Voice Activity Detection) integration\n * - Adaptive quality based on connection\n * - Enhanced error handling and metrics\n */\n\nclass EnhancedVoiceAssistant {\n    constructor(config = {}) {\n        this.config = {\n            wsUrl: config.wsUrl || 'ws://localhost:48231',\n            chunkSize: config.chunkSize || 1024,\n            chunkIntervalMs: config.chunkIntervalMs || 50,\n            sampleRate: config.sampleRate || 16000,\n            enableBinaryAudio: config.enableBinaryAudio !== false,\n            enableVAD: config.enableVAD || false,\n            vadSilenceMs: config.vadSilenceMs || 1500,\n            adaptiveQuality: config.adaptiveQuality !== false,\n            maxRetries: config.maxRetries || 3,\n            enableMetrics: config.enableMetrics !== false,\n            ...config\n        };\n        \n        // State\n        this.ws = null;\n        this.audioContext = null;\n        this.audioWorklet = null;\n        this.mediaStream = null;\n        this.sourceNode = null;\n        this.isRecording = false;\n        this.currentStreamId = null;\n        \n        // Metrics and monitoring\n        this.metrics = {\n            connection: { connected: false, reconnections: 0, lastReconnect: null },\n            latency: { current: 0, average: 0, samples: [] },\n            audio: { chunksSent: 0, totalBytes: 0, binaryChunks: 0 },\n            errors: { count: 0, lastError: null },\n            performance: { workletSupported: false, binarySupported: false }\n        };\n        \n        // Callbacks\n        this.onMessage = null;\n        this.onError = null;\n        this.onConnectionChange = null;\n        \n        this.initPromise = null;\n    }\n    \n    async initialize() {\n        if (this.initPromise) {\n            return this.initPromise;\n        }\n        \n        this.initPromise = this._doInitialize();\n        return this.initPromise;\n    }\n    \n    async _doInitialize() {\n        try {\n            console.log('ðŸš€ Initializing Enhanced Voice Assistant...');\n            \n            // Check for required features\n            await this._checkCapabilities();\n            \n            // Initialize audio context\n            await this._initializeAudio();\n            \n            // Connect WebSocket\n            await this._connectWebSocket();\n            \n            console.log('âœ… Enhanced Voice Assistant initialized successfully');\n            return true;\n            \n        } catch (error) {\n            console.error('âŒ Enhanced Voice Assistant initialization failed:', error);\n            this._recordError(error);\n            throw error;\n        }\n    }\n    \n    async _checkCapabilities() {\n        // Check AudioWorklet support\n        this.metrics.performance.workletSupported = !!(window.AudioContext && AudioContext.prototype.audioWorklet);\n        \n        // Check WebSocket binary support\n        this.metrics.performance.binarySupported = !!(window.WebSocket && WebSocket.prototype.send);\n        \n        // Check MediaStream support\n        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n            throw new Error('MediaDevices API not supported');\n        }\n        \n        console.log('ðŸ“Š Capabilities:', this.metrics.performance);\n    }\n    \n    async _initializeAudio() {\n        // Create AudioContext with optimal settings\n        this.audioContext = new (window.AudioContext || window.webkitAudioContext)({\n            sampleRate: 48000, // Let browser handle, we'll downsample in worklet\n            latencyHint: 'interactive'\n        });\n        \n        if (this.audioContext.state === 'suspended') {\n            await this.audioContext.resume();\n        }\n        \n        // Load Audio Worklet if supported\n        if (this.metrics.performance.workletSupported) {\n            try {\n                await this.audioContext.audioWorklet.addModule('audio-worklet-processor.js');\n                console.log('âœ… Audio Worklet loaded');\n            } catch (error) {\n                console.warn('âš ï¸ Audio Worklet failed to load, using fallback:', error);\n                this.metrics.performance.workletSupported = false;\n            }\n        }\n    }\n    \n    async _connectWebSocket() {\n        return new Promise((resolve, reject) => {\n            try {\n                this.ws = new WebSocket(this.config.wsUrl);\n                \n                this.ws.onopen = () => {\n                    console.log('ðŸ”— WebSocket connected');\n                    this.metrics.connection.connected = true;\n                    this._sendHandshake();\n                    resolve();\n                    this.onConnectionChange?.(true);\n                };\n                \n                this.ws.onclose = (event) => {\n                    console.log('ðŸ”Œ WebSocket disconnected:', event.code, event.reason);\n                    this.metrics.connection.connected = false;\n                    this.onConnectionChange?.(false);\n                    \n                    // Auto-reconnect logic\n                    if (this.config.adaptiveQuality && this.metrics.connection.reconnections < this.config.maxRetries) {\n                        setTimeout(() => this._attemptReconnect(), 2000 * (this.metrics.connection.reconnections + 1));\n                    }\n                };\n                \n                this.ws.onerror = (error) => {\n                    console.error('âŒ WebSocket error:', error);\n                    this._recordError(error);\n                    reject(error);\n                };\n                \n                this.ws.onmessage = (event) => {\n                    this._handleWebSocketMessage(event);\n                };\n                \n                // Connection timeout\n                setTimeout(() => {\n                    if (this.ws.readyState === WebSocket.CONNECTING) {\n                        this.ws.close();\n                        reject(new Error('WebSocket connection timeout'));\n                    }\n                }, 10000);\n                \n            } catch (error) {\n                reject(error);\n            }\n        });\n    }\n    \n    _sendHandshake() {\n        const handshake = {\n            op: 'hello',\n            version: '2.0',\n            capabilities: {\n                binaryAudio: this.config.enableBinaryAudio && this.metrics.performance.binarySupported,\n                audioWorklet: this.metrics.performance.workletSupported,\n                vad: this.config.enableVAD\n            },\n            timestamp: Date.now()\n        };\n        \n        this._sendMessage(handshake);\n    }\n    \n    _handleWebSocketMessage(event) {\n        try {\n            const data = JSON.parse(event.data);
            console.log("[EVA] <-", data);
            if (data.op === "ready") {
                console.log("[EVA] READY â†’ start_audio_stream + ping");
                try {
                    this._sendMessage && this._sendMessage({ type: "start_audio_stream", stream_id: "gui" });
                    this._sendMessage && this._sendMessage({ type: "ping", timestamp: Date.now() });
                } catch (e) { console.warn("[EVA] failed to send start/ping", e); }
            }
            if (data.type === "tts_chunk") {
                console.log(`[EVA] TTS_CHUNK ${data.index+1}/${data.total} engine=${data.engine}`);
            }
            if (data.type === "tts_sequence_end") {
                console.log("[EVA] TTS_SEQUENCE_END", data.sequence_id);
            }\n            \n            // Track latency for ping/pong\n            if (data.type === 'pong' && data.client_timestamp) {\n                const latency = Date.now() - data.client_timestamp;\n                this._updateLatencyMetrics(latency);\n            }\n            \n            // Handle VAD auto-stop\n            if (data.type === 'audio_stream_error' && data.message.includes('VAD auto-stop')) {\n                console.log('ðŸŽ™ï¸ VAD triggered auto-stop');\n                this.stopRecording();\n            }\n            \n            // Pass to callback\n            this.onMessage?.(data);\n            \n        } catch (error) {\n            console.error('Message parsing error:', error);\n            this._recordError(error);\n        }\n    }\n    \n    async startRecording() {\n        if (this.isRecording) {\n            console.warn('Already recording');\n            return false;\n        }\n        \n        try {\n            console.log('ðŸŽ¤ Starting recording...');\n            \n            // Get user media with enhanced constraints\n            const constraints = {\n                audio: {\n                    sampleRate: { ideal: this.config.sampleRate },\n                    channelCount: { ideal: 1 },\n                    echoCancellation: true,\n                    noiseSuppression: true,\n                    autoGainControl: true,\n                    latency: { ideal: 0.01 } // 10ms latency hint\n                },\n                video: false\n            };\n            \n            this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints);\n            \n            // Start audio stream\n            await this._startAudioStream();\n            \n            this.isRecording = true;\n            console.log('âœ… Recording started');\n            return true;\n            \n        } catch (error) {\n            console.error('âŒ Failed to start recording:', error);\n            this._recordError(error);\n            this.onError?.(error);\n            return false;\n        }\n    }\n    \n    async _startAudioStream() {\n        // Request new stream from server\n        const streamRequest = {\n            type: 'start_audio_stream',\n            config: {\n                chunkSize: this.config.chunkSize,\n                sampleRate: this.config.sampleRate,\n                binaryMode: this.config.enableBinaryAudio,\n                vadEnabled: this.config.enableVAD\n            },\n            timestamp: Date.now()\n        };\n        \n        this._sendMessage(streamRequest);\n        \n        // Set up audio processing\n        this.sourceNode = this.audioContext.createMediaStreamSource(this.mediaStream);\n        \n        if (this.metrics.performance.workletSupported) {\n            // Use Audio Worklet for better performance\n            this.audioWorklet = new AudioWorkletNode(this.audioContext, 'audio-streamer-worklet');\n            \n            this.audioWorklet.port.onmessage = (event) => {\n                this._handleWorkletMessage(event.data);\n            };\n            \n            this.sourceNode.connect(this.audioWorklet);\n            \n            // Configure worklet\n            this.audioWorklet.port.postMessage({\n                type: 'start',\n                chunkSize: this.config.chunkSize,\n                binaryMode: this.config.enableBinaryAudio,\n                vadEnabled: this.config.enableVAD\n            });\n            \n        } else {\n            // Fallback to ScriptProcessorNode\n            this._setupScriptProcessor();\n        }\n    }\n    \n    _handleWorkletMessage(data) {\n        switch (data.type) {\n            case 'chunk_binary':\n                this._sendBinaryChunk(data.data, data.sequence);\n                break;\n                \n            case 'chunk_base64':\n                this._sendBase64Chunk(data.data, data.sequence);\n                break;\n                \n            case 'vad_silence_detected':\n                console.log('ðŸ”‡ VAD detected silence, auto-stopping');\n                this.stopRecording();\n                break;\n        }\n    }\n    \n    _sendBinaryChunk(arrayBuffer, sequence) {\n        const message = {\n            type: 'audio_chunk',\n            stream_id: this.currentStreamId,\n            sequence: sequence,\n            is_binary: true,\n            chunk: arrayBuffer,\n            timestamp: Date.now()\n        };\n        \n        // Send binary data directly\n        this.ws.send(JSON.stringify({\n            ...message,\n            chunk: null\n        }));\n        this.ws.send(arrayBuffer);\n        \n        this.metrics.audio.binaryChunks++;\n        this.metrics.audio.chunksSent++;\n        this.metrics.audio.totalBytes += arrayBuffer.byteLength;\n    }\n    \n    _sendBase64Chunk(pcmData, sequence) {\n        // Convert to base64\n        const uint8Array = new Uint8Array(new Int16Array(pcmData).buffer);\n        const base64 = btoa(String.fromCharCode(...uint8Array));\n        \n        const message = {\n            type: 'audio_chunk',\n            stream_id: this.currentStreamId,\n            sequence: sequence,\n            is_binary: false,\n            chunk: base64,\n            timestamp: Date.now()\n        };\n        \n        this._sendMessage(message);\n        \n        this.metrics.audio.chunksSent++;\n        this.metrics.audio.totalBytes += uint8Array.length;\n    }\n    \n    _setupScriptProcessor() {\n        // Fallback audio processing using ScriptProcessorNode\n        const bufferSize = 4096;\n        const processor = this.audioContext.createScriptProcessor(bufferSize, 1, 1);\n        \n        let sequence = 0;\n        let buffer = [];\n        \n        processor.onaudioprocess = (event) => {\n            const inputData = event.inputBuffer.getChannelData(0);\n            \n            // Simple downsampling and buffering\n            for (let i = 0; i < inputData.length; i += 3) { // 48kHz -> 16kHz\n                buffer.push(inputData[i]);\n                \n                if (buffer.length >= this.config.chunkSize) {\n                    const chunk = buffer.splice(0, this.config.chunkSize);\n                    const pcm16 = chunk.map(sample => Math.max(-32768, Math.min(32767, sample * 32768)));\n                    this._sendBase64Chunk(pcm16, sequence++);\n                }\n            }\n        };\n        \n        this.sourceNode.connect(processor);\n        processor.connect(this.audioContext.destination);\n        this.processor = processor;\n    }\n    \n    async stopRecording() {\n        if (!this.isRecording) {\n            return;\n        }\n        \n        console.log('ðŸ›‘ Stopping recording...');\n        \n        this.isRecording = false;\n        \n        // Stop audio worklet\n        if (this.audioWorklet) {\n            this.audioWorklet.port.postMessage({ type: 'stop' });\n            this.audioWorklet.disconnect();\n            this.audioWorklet = null;\n        }\n        \n        // Disconnect audio nodes\n        if (this.sourceNode) {\n            this.sourceNode.disconnect();\n            this.sourceNode = null;\n        }\n        \n        if (this.processor) {\n            this.processor.disconnect();\n            this.processor = null;\n        }\n        \n        // Stop media stream\n        if (this.mediaStream) {\n            this.mediaStream.getTracks().forEach(track => track.stop());\n            this.mediaStream = null;\n        }\n        \n        // Notify server\n        if (this.currentStreamId) {\n            this._sendMessage({\n                type: 'end_audio_stream',\n                stream_id: this.currentStreamId,\n                timestamp: Date.now()\n            });\n            this.currentStreamId = null;\n        }\n        \n        console.log('âœ… Recording stopped');\n    }\n    \n    async sendText(text) {\n        if (!text || !this.ws || this.ws.readyState !== WebSocket.OPEN) {\n            return false;\n        }\n        \n        const message = {\n            type: 'text',\n            content: text,\n            timestamp: Date.now()\n        };\n        \n        this._sendMessage(message);\n        return true;\n    }\n    \n    _sendMessage(message) {\n        if (this.ws && this.ws.readyState === WebSocket.OPEN) {\n            this.ws.send(JSON.stringify(message));\n        }\n    }\n    \n    _updateLatencyMetrics(latency) {\n        this.metrics.latency.current = latency;\n        this.metrics.latency.samples.push(latency);\n        \n        if (this.metrics.latency.samples.length > 100) {\n            this.metrics.latency.samples.shift();\n        }\n        \n        this.metrics.latency.average = this.metrics.latency.samples.reduce((a, b) => a + b, 0) / this.metrics.latency.samples.length;\n    }\n    \n    _recordError(error) {\n        this.metrics.errors.count++;\n        this.metrics.errors.lastError = {\n            message: error.message,\n            timestamp: Date.now()\n        };\n    }\n    \n    async _attemptReconnect() {\n        this.metrics.connection.reconnections++;\n        this.metrics.connection.lastReconnect = Date.now();\n        \n        console.log(`ðŸ”„ Reconnection attempt ${this.metrics.connection.reconnections}`);\n        \n        try {\n            await this._connectWebSocket();\n        } catch (error) {\n            console.error('Reconnection failed:', error);\n        }\n    }\n    \n    getMetrics() {\n        return { ...this.metrics, config: this.config };\n    }\n    \n    async cleanup() {\n        await this.stopRecording();\n        \n        if (this.ws) {\n            this.ws.close();\n            this.ws = null;\n        }\n        \n        if (this.audioContext) {\n            await this.audioContext.close();\n            this.audioContext = null;\n        }\n    }\n}\n\n// Make globally available\nwindow.EnhancedVoiceAssistant = EnhancedVoiceAssistant;\n\n// Export for module systems\nif (typeof module !== 'undefined' && module.exports) {\n    module.exports = EnhancedVoiceAssistant;\n}\n
// Alias fÃ¼r RÃ¼ckwÃ¤rtskompatibilitÃ¤t
window.VoiceAssistant = EnhancedVoiceAssistant;

