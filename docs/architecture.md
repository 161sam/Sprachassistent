# ğŸ— ArchitekturÃ¼bersicht â€“ Sprachassistent

Diese Datei beschreibt die Systemarchitektur des dezentralen Sprachassistenten, der auf Raspberry Pi, Odroid und einem zentralen Server basiert.

---

## ğŸ¯ Ziel

Ein verteilter Sprachassistent, der lokal Sprache versteht (STT), antwortet (TTS) und komplexe Aufgaben an eine zentrale Automatisierungslogik weiterleitet (n8n, Flowise, lokale LLMs).

---

## ğŸ§± Komponenten

### ğŸ”Š Raspi 4 (8 GB)

* **STT**: [`faster-whisper`](https://github.com/guillaumekln/faster-whisper)
* **TTS**: [`piper`](https://github.com/rhasspy/piper)
* **Intent-Routing**: ML-basierte Klassifikation mit Fallback
* **WebSocket-Server**: `ws_server_fastapi.py` (FastAPI-basierter Nachfolger)

### ğŸ§° Raspi 400 (4 GB)

* **GUI**: Electron-/Web-basierte Steuerung Ã¼ber integrierte Tastatur + Display
* **WebSocket-Client**: sendet Sprache an Raspi 4, empfÃ¤ngt Antwort (Text + Audio)

### ğŸ§  Odroid N2

* **Flowise (Docker/NPM)**: Agent-basierte LLM-Antwortlogik
* **n8n (Docker/NPM)**: Automatisierungslogik fÃ¼r Smart-Home, Skills etc.

### â˜ï¸ Optionaler Server

* **lokale LLMs**: z.â€¯B. GGUF/OpenLLM/LLama.cpp
* **zentrale Flowise-/n8n-Instanzen**

---

## ğŸ”€ DatenflÃ¼sse

```mermaid
flowchart LR
  GUI -->|Audio| WS
  WS -->|STT| STT
  STT -->|Text| Intent
  Intent -->|Routing| TTS
  TTS -->|Audio + Text| GUI
```

Der typische Ablauf:

1. **GUI** zeichnet Audio auf und sendet es an den **WebSocket-Server (WS)**.
2. Der WS leitet den Strom an die **STT**-Komponente weiter.
3. Aus der Transkription erkennt die **Intent**-Logik den passenden Skill oder ruft externe Dienste (Flowise/n8n) auf.
4. Die Antwort wird mit **TTS** in Audio umgewandelt.
5. Audio und Text gehen zurÃ¼ck an die **GUI** und werden abgespielt/angezeigt.

---

## ğŸ” Netzwerk & Sicherheit

* **Headscale VPN** verbindet alle GerÃ¤te sicher
* Token-Auth + IP-Filterung im WS-Server
* Optional: HTTPS mit lokalen Zertifikaten

---

## ğŸ§© Erweiterbarkeit

* Plugin-System fÃ¼r Skills
* Weitere GUIs (Mobile, Web)
* Mehrere STT/TTS-Knoten

---

## ğŸ“¦ Projektstruktur (Auszug)

```
cli.sh
config/
docs/
scripts/
ws-server/
voice-assistant-apps/
```

### Standardprofile & `.env`

Profile werden mit `./config/setup_env.sh <profile>` aktiviert und erzeugen eine `.env` im Projektstamm. Wichtige Variablen:

| Variable | Beschreibung |
|----------|--------------|
| `TTS_ENGINE` | Standard TTS Engine (`piper`/`kokoro`) |
| `TTS_MODEL_DIR` | Verzeichnis der TTS-Modelle |
| `FLOWISE_URL` / `N8N_URL` | Endpunkte externer Dienste |

### Empfohlene Verzeichnisse

* `~/models/` â€“ zentrale Modelle (Whisper, Piper, Kokoro)
* `~/.config/` â€“ benutzerspezifische Einstellungen

*Pfadvariablen kÃ¶nnen in der `.env` Ã¼berschrieben werden.*

---

## ğŸ“Œ NÃ¤chste Schritte

* Skill-System dokumentieren (`docs/skill-system.md`)
* Netzwerk-Setup in `docs/headscale-setup.md`
* Routing-Logik in `docs/routing.md`

